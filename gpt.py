# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rctEqyNLG1vuWvf3xJTX-edUlqrFWjWy
"""

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
device=('cuda' if torch.cuda.is_available()  else 'cpu')

with open('poem.txt','r') as file:
  txt=file.read()
txt=txt.lower()
len(txt)

chars=sorted(set(txt))
vocab_size=len(chars)
n_embd=96

stoi={s:i for i,s in enumerate(chars)}
itos={i:s for s,i in stoi.items()}
encode=lambda s:[stoi[string.lower()]  for string in s]
decode=lambda l:''.join([itos[n] for n in l])

data=torch.tensor(encode(txt),dtype=torch.long,device=device)
n=int(0.8*len(data))
train_data=data[:n]
test_data=data[n:]

context_size=64
batch_size=32
def getbatch(x):
  data=train_data if x=='train' else test_data
  idx=torch.randint(len(data)-context_size,(batch_size,))
  x=torch.stack([data[n:n+context_size] for n in idx])
  y=torch.stack([data[n+1:n+context_size+1] for n in idx])
  return x,y

x,y=getbatch('train')

head_size=n_embd//12
class head(nn.Module):
  def __init__(self):
    super().__init__()
    B,T,C=batch_size,context_size,n_embd
    self.key=nn.Linear(C,head_size,bias=False)
    self.query=nn.Linear(C,head_size,bias=False)
    self.value=nn.Linear(C,head_size,bias=False)
    self.register_buffer('tril',torch.tril(torch.ones(T,T)))

  def forward(self,x):
    B,T,C=x.shape
    k=self.key(x)
    q=self.query(x)
    v=self.value(x)
    wei=q@k.transpose(-2,-1)*C**-0.5
    wei=wei.masked_fill(self.tril[:T,:T]==0 ,float('-inf'))
    wei=F.softmax(wei,dim=-1)
    out=wei@v
    return out

num_heads = 12

class multiheadattention(nn.Module):
    def __init__(self):
        super().__init__()
        self.sa_head = nn.ModuleList([head() for _ in range(num_heads)])
        self.proj=nn.Linear(n_embd,n_embd)

    def forward(self, x):
        out=torch.cat([h(x) for h in self.sa_head], dim=-1)
        return self.proj(out)

class forwardmethod(nn.Module):
  def __init__(self):
      super().__init__()
      self.ffwd=nn.Sequential(
            nn.Linear(in_features=n_embd,out_features=4*n_embd),
            nn.ReLU(),
            nn.Linear(4*n_embd,n_embd)
        )
  def forward(self,x):
    return self.ffwd(x)

class Block(nn.Module):
  def __init__(self,n_embd,num_head):
    super().__init__()
    head_size=n_embd//num_head
    self.sa=multiheadattention()
    self.ffwd=forwardmethod()
    self.ln1=nn.LayerNorm(n_embd)
    self.ln2=nn.LayerNorm(n_embd)

  def forward(self,x):
    x=x+self.sa(self.ln1(x))
    x=x+self.ffwd(self.ln2(x))
    return x

class gpt(nn.Module):
  def __init__(self):
    super().__init__()
    self.embd=nn.Embedding(vocab_size,n_embd)
    self.pos=nn.Embedding(context_size,n_embd)
    self.shead=multiheadattention()
    self.lm_head=nn.Linear(n_embd,vocab_size)
    self.ffwd=forwardmethod()
    self.blocks=nn.Sequential(
        Block(n_embd,num_heads),
        Block(n_embd,num_heads),
        Block(n_embd,num_heads),
        nn.LayerNorm(n_embd)
    )

  def forward(self,x,targets=None):
    B,T=x.shape
    tok_embd=self.embd(x)
    pos_embd=self.pos(torch.arange(T,device=device))
    x=tok_embd+pos_embd
    x=self.blocks(x)
    logits=self.lm_head(x)
    if targets is None:
      loss=None

    else:

      B,T,C=logits.shape
      logits=logits.view(B*T,C)
      targets=targets.view(B*T)
      loss=F.cross_entropy(logits,targets)
    return logits,loss

  def generate(self,context,max_new_token):
    for _ in range(max_new_token):
      idx_cond=context[:,-context_size:]
      logit,loss=self(idx_cond)
      logit=logit[:,-1,:]
      probs=F.softmax(logit,dim=-1)
      idx_next=torch.multinomial(probs,num_samples=1)
      context=torch.cat((context,idx_next),dim=1)
    return context
model=gpt().to(device)

optimizer=torch.optim.Adam(model.parameters(),lr=1e-3)

for epoch  in range(1000):
  xb,yb=getbatch('train')
  logits,loss=model(xb,yb)
  optimizer.zero_grad(set_to_none=True)
  loss.backward()
  optimizer.step()
print(loss.item())

def gen(text):
  print(decode(model.generate(torch.tensor(encode(text),dtype=torch.long,device=device).unsqueeze(dim=0),10000)[0].tolist()))

gen('it was a rainy day')